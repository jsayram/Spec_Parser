# POCT1 Spec Parser Configuration
# Copy this file to .env and customize as needed

# Output directories
OUTPUT_DIR=./output
IMAGE_DIR=./output/images
MARKDOWN_DIR=./output/markdown
JSON_DIR=./output/json
INDEX_DIR=./output/indices

# OCR settings
OCR_LANGUAGE=eng
OCR_DPI=300
OCR_CONFIDENCE_THRESHOLD=0.7

# Embedding settings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DEVICE=cpu
EMBEDDING_BATCH_SIZE=32

# Search settings
SEARCH_TOP_K=5
HYBRID_SEARCH_ALPHA=0.5

# RLM settings (for surgical extraction)
RLM_CONTEXT_WINDOW=2000
RLM_MAX_SPAN_LENGTH=5000
RLM_NEIGHBORS_COUNT=3

# LLM Provider settings
LLM_PROVIDER=ollama                    # Options: "huggingface", "ollama", "anthropic", "openai"
LLM_MODEL=qwen2.5-coder:7b             # Model identifier (e.g., "Qwen/Qwen2.5-Coder-7B-Instruct", "gpt-4o")
LLM_BASE_URL=http://localhost:11434   # Ollama base URL (local only)
LLM_TEMPERATURE=0.0                    # 0.0 = deterministic, >0.0 = creative
LLM_MAX_TOKENS=4000                    # Maximum response tokens
LLM_RATE_LIMIT=1.0                     # External API rate limit (requests per second)
LLM_TIMEOUT=120                        # Request timeout in seconds

# LLM Cache settings
LLM_CACHE_DIR=config                   # Directory for correction cache database
LLM_GLOBAL_CACHE=llm_corrections.db    # Global corrections database filename

# API Keys (SENSITIVE - never commit these!)
# For Anthropic Claude API
ANTHROPIC_API_KEY=

# For OpenAI API
OPENAI_API_KEY=

# HuggingFace Configuration (optional)
# Models cached at ~/.cache/huggingface by default
# HF_HOME=/path/to/model/cache
# HF_TOKEN=  # Only needed for gated models

# Logging
LOG_LEVEL=INFO
# LOG_FILE=./logs/spec_parser.log

# Performance
MAX_WORKERS=4
